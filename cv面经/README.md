# 1.过拟合如何解决?
* 获取和使用更多的数据集 <br>
对于解决过拟合的办法就是给与足够多的数据集，让模型在更可能多的数据上进行“观察”和拟合，从而不断修正自己。然而事实上，收集无限多的数据集几乎是不可能的，因此一个常用的办法就是调整已有的数据，添加大量的“噪音”，或者对图像进行锐化、旋转、明暗度调整等优化. <br>
* 采用合适的模型 <br>
* 使用 Dropout <br>
* 正则化 <br>
正则化又称为权重衰减，具体做法是将权值的大小加入到损失函数中，在实际使用中分为L1正则与L2正则化. <br>
[正则化](正则化.md) <br>
* 可变化的学习率 <br>
可变化的学习率也是根据模型计算出的准确率进行调整。一个简单的方法是在人为设定的准确率范围内，达到10次范围内的波动后，依次将学习率减半，直到最终的学习率降为原始的 1/1024 时停止模型的训练. <br>
* 使用 Batch_Normalization <br>
还有一个数据处理的方法 Batch_Normalization，即数据在经过卷积层之后，真正进入激活函数之前需要对其进行一次 Batch_Normalization，分批对输入的数据求取均值和方差之后重新对数据进行归一化计算。
这样做的好处就是对数据进行一定程度的预处理，使得无论是训练集还是测试集都在一定范围内进行分布和波动，对数据点中包含的误差进行掩盖化处理，从而增大模型的泛化能力. <br>

# 2.1*1卷积的作用
1.实现跨通道的交互和信息整合 <br>
2.进行卷积核通道数的降维和升维 <br>
3.实现多个feature map的线性组合,实现通道个数的变换 <br>
4.对特征图进行一个比例缩放 <br>

# 3.梯度爆炸与消失的原因,以及如何解决?
[梯度爆炸与消失](梯度爆炸与消失.md) <br>

# 4.Batch Normalization 和 Group Normalization有何区别？
BN是在 batch这个维度上进行归一化，GN是计算channel方向每个group的均值方差. <br>

# 5.深度机器学习中的mini-batch的大小对学习效果有何影响？
mini-batch太小会导致收敛变慢，太大容易陷入sharp minima，泛化性不好. <br>

# 6.为什么SSD对小目标检测效果不好?
(1) 小目标对应的anchor比较少，其对应的feature map上的pixel难以得到训练，这也是为什么SSD在augmentation之后精确度上涨（因为crop之后小目标就变为大目标）<br>
(2) 要检测小目标需要足够大的feature map来提供精确特征，同时也需要足够的语义信息来与背景作区 <br>

# 7. 如何理解卷积、池化等、全连接层等操作 
(1)卷积的作用：捕获图像相邻像素的依赖性；起到类似滤波器的作用，得到不同形态的feature map <br>
(2)激活函数的作用：引入非线性因素<br>
(3)池化的作用：减少特征维度大小，使特征更加可控；减少参数个数，从而控制过拟合程度；增加网络对略微变换后的图像的鲁棒性；达到一种尺度不变性，即无论物体在图像中哪个方位均可以被检测到 <br>

# 8. softmax
https://zhuanlan.zhihu.com/p/25723112 <br>

# 9. 损失函数
 [激活函数与损失函数](../激活函数与损失函数) <br>
