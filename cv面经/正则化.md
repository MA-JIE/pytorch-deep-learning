# L2正则化
权重衰减等价于L2范数正则化(regularization)。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小,是应对过拟合的常用手段。我们先描述L2范数正则化,再解释它为何又称权重衰减。<br>
L2范数正则化在模型原损失函数基础上添加L2范数惩罚项,从而得到训练所需要最小化的函数. L2范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积. 下面是普通的损失函数: <br>
![loss](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/cv%E9%9D%A2%E7%BB%8F/img/loss1.png) <br>
其中w1, w2是权重参数, b是偏差参数,样本i的输入为x1,x2 ,标签为y(i), 样本数为n。将权重参数用向量w = [w1 , w2 ]表示,带有L2范数惩罚项的新损失函数为: <br>
![loss](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/cv%E9%9D%A2%E7%BB%8F/img/loss2.png) <br>
其中超参数λ > 0。当权重参数均为0时,惩罚项最小。当λ较大时,惩罚项在损失函数中的比重较大,这通常会使学到的权重参数的元素较接近0。当λ设为0时,惩罚项完全不起作用。上式中L2范数平方||w||^2 展开后得到w 1 2 + w 2 2 。有了L 2 范数惩罚项后,在小批量随机梯度下降中,w1,w2的迭代方式更改为: <br>
![loss](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/cv%E9%9D%A2%E7%BB%8F/img/loss3.png) <br>
可⻅,L2范数正则化令权重w1和w2先自乘小于1的数,再减去不含惩罚项的梯度。因此,L2范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制,这可能对过拟合有效。实际场景中,我们有时也在惩罚项中添加偏差元素的平方和.<br>

# L1正则化
L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择,一定程度上，L1也可以防止过拟合. <br>

