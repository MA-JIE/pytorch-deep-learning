# 梯度不稳定
首先让我们先来了解一个概念：什么是梯度不稳定呢？<br>
概念：在深度神经网络中的梯度是不稳定的，在靠近输入层的隐藏层中或会消失，或会爆炸。这种不稳定性才是深度神经网络中基于梯度学习的根本问题。<br>
产生梯度不稳定的根本原因：前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸。<br>
划重点：梯度消失和梯度爆炸属于梯度不稳定的范畴 <br>

# 梯度爆炸与消失的概念
梯度消失：因为通常神经网络所用的激活函数是sigmoid函数，这个函数有个特点，就是能将负无穷到正无穷的数映射到0和1之间，
并且对这个函数求导的结果是f′(x)=f(x)(1−f(x))。因此两个0到1之间的数相乘，得到的结果就会变得很小了。
神经网络的反向传播是逐层对函数偏导相乘，因此当神经网络层数非常深的时候，最后一层产生的偏差就因为乘了很多的小于1的数而越来越小，
最终就会变为0，从而导致层数比较浅的权重没有更新，这就是梯度消失. <br>
梯度爆炸：当初始的权值过大，靠近输入层的hidden layer1的权值变化比靠近输出层的hidden layer3的权值变化更快，就会引起梯度爆炸的问题。
在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，
权重的值变得非常大，以至于溢出，导致 NaN 值。网络层之间的梯度（值大于1.0）重复相乘导致的指数级增长会产生梯度爆炸.<br>
# 梯度爆炸与消失的解决办法
* 预训练加微调 <br>
* 梯度剪切、正则 <br>
梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内.这可以防止梯度爆炸.<br>
另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是l1正则，和l2正则，在各个深度框架中都有相应的API可以使用正则化. <br>
* relu、leakrelu、elu等激活函数 <br>
激活函数详解可看激活函数部分. <br>
relu: 思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生. <br>
relu的主要贡献:<br>
(1) 解决了梯度消失、爆炸的问题 <br>
(2) 计算方便，计算速度快 <br>
(3) 加速了网络的训练 <br>
relu的缺点: <br>
(1) 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
(2) 输出不是以0为中心的 <br>
leakrelu: <br>
leakrelu就是为了解决relu的0区间带来的影响，其数学表达为：leakrelu=max(k∗x,x),其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来.<br>
* BN
正向传播中f2=f1(wT∗x+b)，那么反向传播中，∂f2 / ∂w=∂f2 / ∂f1 x ，反向传播式子中有x的存在，所以x的大小影响了梯度的消失和爆炸. <br>
batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了x带来的放大缩小的影响，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区. <br>

