激活函数
-------
#### 为什么会用到激活函数？
首先数据的分布绝大多数是非线性的，而一般神经网络的计算是线性的，引入激活函数，是在神经网络中引入非线性，强化网络的学习能力。所以激活函数的最大特点就是非线性。
#### 为什么会存在多种激活函数？
Sigmoid和tanh的特点是将输出限制在(0,1)和(-1,1)之间，说明Sigmoid和tanh适合做概率值的处理，例如LSTM中的各种门；而ReLU就不行，因为ReLU无最大值限制，可能会出现很大值。同样，根据ReLU的特征，Relu适合用于深层网络的训练，而Sigmoid和tanh则不行，因为它们会出现梯度消失。
#### 使用relu激活函数时，是否还存在梯度消失的问题？
梯度衰减因子包括激活函数导数，此外，还有多个权重连乘也会影响。梯度消失只是表面说法，按照这样理解，底层使用非常大的学习率，或者人工添加梯度噪音，原则上也能回避，有不少论文这样试了，然而目前来看，有用，但没太大的用处。
#### sigmoid
sigmoid函数也称为Logistic函数，因为Sigmoid函数可以从Logistic回归（LR）中推理得到，也是LR模型指定的激活函数。<br>
sigmod函数的取值范围在（0, 1）之间，可以将网络的输出映射在这一范围，方便分析。<br>
函数表达式以及导数表达式：<br>


