NiN(Network in Network)
================
paper:<br>
https://arxiv.org/abs/1312.4400 <br>

# 1 x 1卷积核
NIN网络是第一个提出1×1卷积核的论文,那么1×1卷积核有什么作用呢，如果当前层和下一层都只有一个通道那么1×1卷积核确实没什么作用，但是如果它们分别为m层和n层的话，1×1卷积核可以起到一个跨通道聚合的作用，所以进一步可以起到降维（或者升维）的作用，起到减少参数的目的。比如当前层为 x × x × m 即图像大小为x × x，特征层数为m，然后如果将其通过1×1的卷积核，特征层数为n，那么只要n<m这样就能起到降维的目的，减少之后步骤的运算量（当然这里不太严谨，需要考虑1×1卷积核本身的参数个数为m×n个)。换句话说，如果使用1x1的卷积核，这个操作实现的就是多个feature map的线性组合，可以实现feature map在通道个数上的变化。而因为卷积操作本身就可以做到各个通道的重新聚合的作用，所以1×1的卷积核也能达到这个效果。<br>
* 降维 <br>
* 增加非线性 <br>
1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。<br>
* 跨通道信息交互（channal 的变换),例子：使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互.<br>

# NiN块
NiN使用1×1卷积层来替代全连接层,从而使空间信息能够自然传递到后面的层中去,下图对比了NiN同AlexNet和VGG等网络在结构上的主要区别(右边为NiN结构).<br>
![NiN](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/NiN/img/NiN.png) <br>
NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的1 × 1卷积层串联而成。其中第一个卷积层的超参数可以自行设置,而第二和第三个卷积层的超参数一般是固定的。<br>

# NiN模型
NiN是在AlexNet问世不久后提出的。它们的卷积层设定有类似之处。NiN使用卷积窗口形状分别为11 × 11、5 × 5和3 × 3的卷积层,相应的输出通道数也与AlexNet中的一致。每个NiN块后接一个步幅为2、窗口形状为3 × 3的最大池化层。除使用NiN块以外,NiN还有一个设计与AlexNet显著不同:NiN去掉了AlexNet最后的3个全连接层,取而代之地,NiN使用了输出通道数等于标签类别数的NiN块,然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN的这个设计的好处是可以显著减小模型参数尺寸,从而缓解过拟合。然而,该设计有时会造成获得有效模型的训练时间的增加。<br>
