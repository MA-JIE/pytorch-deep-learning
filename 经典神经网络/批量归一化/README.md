# 批量归一化层(batch normalization)
BN层能让较深的神经网络的训练变得更加容易,对于输入数据,我们一般会做标准化处理 处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近:这往往更容易训练出有效的模型.<br>
但对深层神经网络来说,即使输入数据已做标准化,训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。<br>
批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时,批量归一化利用小批量上的均值和标准差,不断调整神经网络中间输出,从而使整个神经网络在各层的中间输出的数值更稳定. <br>
#### 全连接层
批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为u,权重参数和偏差参数分别为W 和b,激活函数为φ。设批量归一化的运算符为BN。那么,使用批量归一化的全连接层的输出为:<br>
φ(BN(x))<br>
x = Wu + b <br>
考 虑 一 个 由m个 样 本 组 成 的 小 批 量, 仿 射 变 换 的 输 出 为 一 个 新 的 小 批 量 B = {x(1) , . . . , x(m) }。它们正是批量归一化层的输入。对于小批量B中任意样本x(i) ∈ Rd , 1 ≤ i ≤ m,批量归一化层的输出同样是d维向量:<br>
y(i) = BN(x(i)), <br>
* 首先,对小批量B求均值和方差:
![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn1.png)<br>


![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn2.png)<br>

![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn3.png)<br>


#### 卷积层
