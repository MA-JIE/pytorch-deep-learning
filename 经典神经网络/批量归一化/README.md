# 批量归一化层(batch normalization)
BN层能让较深的神经网络的训练变得更加容易,对于输入数据,我们一般会做标准化处理 处理后的任意一个特征在数据集中所有样本上的均值为0、标准差为1。标准化处理输入数据使各个特征的分布相近:这往往更容易训练出有效的模型.<br>
但对深层神经网络来说,即使输入数据已做标准化,训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。<br>
批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时,批量归一化利用小批量上的均值和标准差,不断调整神经网络中间输出,从而使整个神经网络在各层的中间输出的数值更稳定. <br>
#### 全连接层
批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为u,权重参数和偏差参数分别为W 和b,激活函数为φ。设批量归一化的运算符为BN。那么,使用批量归一化的全连接层的输出为:<br>
φ(BN(x))<br>
x = Wu + b <br>
考 虑 一 个 由m个 样 本 组 成 的 小 批 量, 仿 射 变 换 的 输 出 为 一 个 新 的 小 批 量 B = {x(1) , . . . , x(m) }。它们正是批量归一化层的输入。对于小批量B中任意样本x(i) ∈ Rd , 1 ≤ i ≤ m,批量归一化层的输出同样是d维向量:<br>
y(i) = BN(x(i)), <br>
* 首先,对小批量B求均值和方差:<br>
![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn1.png)<br>
其中的平方计算是按元素求平方。接下来,使用按元素开方和按元素除法对x(i)标准化: <br>
![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn2.png)<br>
这里ε > 0是一个很小的常数,保证分母大于0。在上面标准化的基础上,批量归一化层引入了两个可以学习的模型参数,拉伸(scale)参数γ和偏移(shift)参数β。这两个参数和x(i)形状相同,皆为d维向量。它们与x̂(i)分别做按元素乘法(符号⊙)和加法计算:<br>
![bn](https://github.com/MA-JIE/pytorch-deep-learning/blob/master/%E7%BB%8F%E5%85%B8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/img/bn3.png)<br>
至此,我们得到了x(i)的批量归一化的输出y(i).值得注意的是,可学习的拉伸和偏移参数保留了不对x(i)做批量归一化的可能:此时只需学出γ = sqrt(σ^2 + ε)和β=μ。我们可以对此这样理解:如果批量归一化无益,理论上,学出的模型可以不使用批量归一化。<br>
#### 卷积层
对卷积层来说,批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道,我们需要对这些通道的输出分别做批量归一化,且每个通道都拥有独立的拉伸和偏移参数,并均为标量。设小批量中有m个样本。在单个通道上,假设卷积计算输出的高和宽分别为p和q。我们需要对该通道中m × p × q个元素同时做批量归一化。对这些元素做标准化计算时,我们使用相同的均值和方差,即该通道中m × p × q个元素的均值和方差。<br>
#### 预测时的批量归一化
使用批量归一化训练时,我们可以将批量大小设得大一点,从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时,我们希望模型对于任意输入都有确定的输出。因此,单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差,并在预测时使用它们得到确定的输出。可⻅,和丢弃层一样,批量归一化层在训练模式和预测模式下的计算结果也是不一样的。<br>
